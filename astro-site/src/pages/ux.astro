---
import BaseLayout from "@layouts/BaseLayout.astro";

const nav = [
  { href: "/", label: "Overview" },
  { href: "/architecture", label: "Architecture" },
  { href: "/hardware", label: "Hardware Model" },
  { href: "/ux", label: "User Journey" },
  { href: "/noise", label: "Noise & QEC" },
  { href: "/implementation", label: "Implementation" },
  { href: "/open-questions", label: "Open Questions" },
];
---
<BaseLayout
  title="User Journeys"
  description="Persona-driven stories: how algorithm authors, compiler engineers, and operators experience the VM."
  nav={nav}
  page="ux"
>
  <section>
    <h1>From kernel to measurements</h1>
    <p>
      <strong>docs/ux.md</strong> describes how different users should experience the Neutral Atom VM. This page
      turns those flows into concrete stories backed by the current Python SDK, CLI, and C++ implementation.
    </p>
  </section>

  <section>
    <h2>Personas at a glance</h2>
    <div class="grid">
      <div class="card">
        <h3>Algorithm authors</h3>
        <p>
          Write Bloqade/Squin kernels and expect device-like ergonomics: <code>connect_device</code>,
          <code>submit</code>, and <code>job.result()</code>, with realistic geometry and noise but no C++.
        </p>
      </div>
      <div class="card">
        <h3>Compiler &amp; tooling engineers</h3>
        <p>
          Target the VM dialect from Kirin or other passes and need a stable ISA plus clear diagnostics when
          schedules or hardware constraints are violated.
        </p>
      </div>
      <div class="card">
        <h3>Infrastructure &amp; ops</h3>
        <p>
          Deploy the VM as a service, manage devices and profiles, and monitor jobs, logs, and errors across
          multiple backends.
        </p>
      </div>
    </div>
  </section>

    <section>
      <h2>Algorithm author journey (Python SDK)</h2>
<pre><code class="language-python">from bloqade import squin
from neutral_atom_vm import connect_device

@squin.kernel
def ghz():
    q = squin.qalloc(3)
    squin.h(q[0])
    squin.cx(q[0], q[1])
    squin.cx(q[0], q[2])
    squin.measure(q)

dev = connect_device("state-vector", profile="ideal_small_array")
result = dev.submit(ghz, shots=1000).result()
print(result["measurements"])
</code></pre>
    <p>
      The <code>connect_device</code> helper lives in <code>python/src/neutral_atom_vm/device.py</code>. It lowers
      kernels via <code>to_vm_program</code>, validates blockade constraints, and builds a structured
      <code>JobRequest</code> with hardware coordinates and optional <code>SimpleNoiseConfig</code>. Profiles are
      discoverable through <code>available_presets()</code> so users pick logical devices instead of raw geometry arrays.
    </p>
    <p>
      From an algorithm author’s perspective, the VM “just looks like hardware”: you select a device/profile, submit
      a kernel, and get measurements and logs back. Everything else (ISA versioning, noise configuration, layout
      details) stays behind the SDK.
    </p>
    <div class="card" style="margin-top: 1rem;">
      <h4>Timing-limit example</h4>
      <p>
        The <code>benchmark_chain</code> profile enforces a 50 microsecond (50,000 ns) measurement cooldown. The scheduler now tracks
        `logical_time` and inserts just enough idle time (either via explicit <code>&lt;Wait&gt;</code> inserts or by relying on gate durations)
        so the kernel below now runs without an explicit <code>Wait</code>.
      </p>
      <pre><code class="language-python">from bloqade import squin

@squin.kernel
def reuse_measured_qubit():
    q = squin.qalloc(3)
    squin.h(q[0])
    squin.measure(q[0])
    squin.h(q[0])  # scheduler inserts the required idle time for the cooldown
</code></pre>
      <p>
        Running this kernel with <code>connect_device("state-vector", profile="benchmark_chain")</code> now
        completes successfully. The JSON/CLI logs document the inserted <code>&lt;Wait&gt;</code> entry (or the gate durations) that bumped
        `logical_time` before the second `h`. If you still want to enforce a larger gap for external timing reasons,
        `squin.wait()` remains available and the scheduler treats it as additional idle time.
      </p>
      <p>
        To see the scheduler in action, run the shipped kernel in <code>python/examples/benchmark_cooldown_violation.py</code>:
      </p>
    <pre><code>quera-vm run \
  --device state-vector \
  --profile benchmark_chain \
  --shots 1 \
  python/examples/benchmark_cooldown_violation.py</code></pre>
      <p>
        The CLI still submits the same job request as the SDK, but instead of a runtime error, the `logs`
        array now includes the scheduler-inserted <code>&lt;Wait&gt;</code> entry right after the measurement. You can capture that stream
        via `--output json` or `--log-file` to audit how the scheduler aligned the program with the hardware cooldown.
      </p>
      <p>
        You can also see how parallel limits shape a schedule. The snippet below drives two disjoint qubit pairs on
        <code>noisy_square_array</code>. When <code>max_parallel_two_qubit</code> is 1, the second <code>CX</code> starts after
        the first finishes; if the limit is raised to 2, the gates share the same timestamp.
      </p>
<pre><code class="language-python">program = [
    &#123;"op": "AllocArray", "n_qubits": 4&#125;,
    &#123;"op": "ApplyGate", "name": "H", "targets": [0]&#125;,
    &#123;"op": "ApplyGate", "name": "H", "targets": [2]&#125;,
    &#123;"op": "ApplyGate", "name": "CX", "targets": [0, 1]&#125;,
    &#123;"op": "ApplyGate", "name": "CX", "targets": [2, 3]&#125;,
    &#123;"op": "Measure", "targets": [0, 1, 2, 3]&#125;,
]</code></pre>
      <p>
        The CLI timeline shows the scheduler’s choices:
      </p>
<pre><code>Timeline (us):
        0.000–    0.500 H[0]
        0.000–    0.500 H[2]
        0.500–    1.500 CX[0,1]
        1.500–    2.500 CX[2,3]  # shifted because the two-qubit cap is 1
        2.500–   52.500 Measure[0,1,2,3]
</code></pre>
      <p>
        When the profile permits two concurrent two-qubit gates, the last two rows collapse to the same start time. The
        notebook timeline widget mirrors the same layout with colored bars, so users can visually confirm which operations
        overlapped and why the scheduler inserted any idle slices.
      </p>
    </div>
  </section>

  <section>
    <h2>Developer journey (CLI)</h2>
    <p>
      The <code>quera-vm</code> CLI (entry point in <code>python/pyproject.toml</code>, implementation in
      <code>python/src/neutral_atom_vm/cli.py</code>) mirrors the SDK flow while exposing extra controls (threads,
      profile JSON overrides, log streaming, remote services). Example invocation:
    </p>
<pre><code>quera-vm run \
  --device state-vector \
  --profile benchmark_chain \
  --shots 1000 \
  --output json \
  examples/ghz.py
</code></pre>
    <p>
      When Stim support is enabled (<code>-DNA_VM_WITH_STIM=ON</code>, now the default) you can pass
      <code>--device stabilizer</code> (or <code>connect_device("stabilizer", ...)</code>) to run Clifford/Pauli workloads on the Stim-backed engine.
      The SDK detects kernels that call documented helpers such as <code>squin.single_qubit_pauli_channel</code> /
      <code>squin.two_qubit_pauli_channel</code>, injects the resulting Stim circuit into the job, and skips rebuilding
      those noise channels from <code>SimpleNoiseConfig</code>. CLI logs include categories like <code>Noise</code> and
      <code>TimingConstraint</code>, so timing violations and noise events are visible without digging into backends.
    </p>
    <p>
      Kernels that rely on explicit squin noise helpers are now treated as simulation-only artifacts: they run on
      <code>--device stabilizer</code> and raise a clear error on hardware-style devices (e.g., <code>state-vector</code>). That prevents
      accidental submission of noise-annotated programs to real hardware while still letting you tune loss/Pauli profiles
      for Stim-based analysis.
    </p>
    <p>
      Each job now returns a structured `timeline` array (alongside the `logs` and `measurements`) that lists the
      scheduled start time, duration, operation name, and a short detail string for every instruction. That lets you
      inspect the planned schedule directly from the CLI or SDK output. Timeline/log timestamps are expressed in
      microseconds and the JSON includes `timeline_units` / `log_time_units` metadata so downstream tools can label axes
      correctly even though the VM continues to track logical time in nanoseconds internally.
    </p>
    <p>
      For local experiments, you can also pass <code>--profile-config</code> to load a JSON profile and override the
      built-in presets. This lets developers iterate on geometry and noise while keeping the rest of the stack fixed.
    </p>
  </section>

  <section>
    <h2>Operator journey (service / ops)</h2>
    <p>
      The same job schema and device/profile model extend naturally to a service. The <code>RemoteServiceError</code>
      plumbing in <code>python/src/neutral_atom_vm/service_client.py</code>
      shows how <code>quera-vm</code> can talk to the VM service over HTTP.
    </p>
    <p>
      Jobs carry device IDs, profile metadata, ISA versions, hardware limits, and programs. The service enqueues work,
      dispatches to an engine, streams measurements/logs, and surfaces structured error codes for violations
      (invalid gates, resource exhaustion, backend failures).
    </p>
    <p>
      A dedicated <code>GET /devices</code> endpoint exposes the same catalog as <code>available_presets()</code>,
      so notebooks and dashboards can dynamically populate dropdowns without shipping their own preset list.
    </p>
  </section>

  <section>
    <h2>Discoverability &amp; transparency</h2>
    <ul>
      <li><strong>Device catalog:</strong> <code>available_presets()</code> exposes geometry/noise metadata and persona labels.</li>
      <li><strong>Logs:</strong> Execution logs capture gate, measurement, noise, and timing categories per shot.</li>
      <li><strong>Diagnostics:</strong> Blockade and geometry checks happen both in the SDK and inside <code>StatevectorEngine</code>.</li>
      <li><strong>Error surfacing:</strong> Unsupported ISA versions and invalid qubit indices raise explicit errors propagated through <code>JobResult.message</code>.</li>
      <li><strong>Histograms:</strong> The result widgets now render `-1` bits as loss with an `L` prefix (e.g., `L01`), highlighting erasure-heavy outcomes from both VM and stabilizer runs.</li>
      <li><strong>End-to-end demo:</strong> the workflow script in <code>python/README.md</code> gives a CLI-free local tour of the full path from kernel to measurements.</li>
    </ul>
  </section>
</BaseLayout>
